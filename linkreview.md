| Название | Ссылка | Краткое описание | Релевантность |
|----------|--------|------------------|---------------|
| Learning Transferable Visual Models From Natural Language Supervision | [link] (https://arxiv.org/pdf/2103.00020) | Фундаментальная статья про встраивание текстовых и визуальных признаков с помощью контрастивного обучения на изображениях и их описаниях. В статье можно использовать её результат в виде модели CLIP, как модель векторизации текста и изображений | ✅ |
| Towards Explainable and Interpretable Multimodal Large Language Models: A Comprehensive Survey | [link] (https://arxiv.org/pdf/2412.02104v1) | Большая обзорная статья про интерпретируемость и объяснимость мультимодальных моделей. В статье рассмотрены методы, метрики, задачи и будущие проблемы на конец 2024 | ✅ |
| Rationalization for Explainable NLP: A Survey | [link] (https://arxiv.org/pdf/2301.08912) | Обзорная статья методов рационализации (когда модель, кроме решения задачи, добавляет объяснения, почему это решение было сделано). Rational AI является подполем Explainable AI, можно попробовать использовать методы рационализации для повышения интерпретируемости NLP моделей | ✅ |
| Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization | [link] (https://openaccess.thecvf.com/content_ICCV_2017/papers/Selvaraju_Grad-CAM_Visual_Explanations_ICCV_2017_paper.pdf) | Статья про метод построения карт локализации для различных задач (классификации, сегментации, VQA и др.), которые используют градиенты предыдущего слоя CNN. Можно использовать в качестве еще одного способа локализации предсказаний CNN | ✅ |
| An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale | [link] (https://arxiv.org/pdf/2010.11929) | Статья, в которой представлена архитектура Vision Tranformer (ViT), который открыл путь использования трансформерных архитектур для изображений | ✅ |
| "Why Should I Trust You?": Explaining the Predictions of Any Classifier | [link] (https://arxiv.org/pdf/1602.04938) | Статья, в которой представлен один из основных методов в XAI - LIME для выделения значимые признаки для предсказания (текстовые и визуальные классификаторы) | ✅ |
| A Unified Approach to Interpreting Model Predictions | [link] (https://arxiv.org/pdf/1705.07874) | Статья, опирающаяся на статью с LIME, которая дает математическую основу объяснения моделей и помогает лучше понять, какие признаки влияют на модель. Базовый метод будет трудно использовать при огромных размерах модели, так как требуется делать много forward-проходов, что затратно | ✅ |
| Exploring Visual Explanations for Contrastive Language-Image Pre-training | [link] (https://arxiv.org/pdf/2209.07046) | Исследование объяснимости моделей CLIP, визуализация влияния изображений и текста на предсказания | ✅ |
| Viziometrics: Analyzing Visual Information in the Scientific Literature | [link] (https://jevinwest.org/papers/Lee2016arxiv.pdf) | Анализ использования визуальной информации (графиков, диаграмм) в научных статьях, оценка значимости для научных выводов | ✅ |
| Learning to Rank: From Pairwise Approach to Listwise Approach | [link] (https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-2007-40.pdfh) | Фундаментальная работа по Learning-to-Rank, сравнение парного и листового подходов | ✅ |
| Multimodal Label Relevance Ranking via Reinforcement Learning | [link] (https://www.ecva.net/papers/eccv_2024/papers_ECCV/papers/08369.pdf) | Использование RL для ранжирования мультимодальных объектов (изображение+текст) | ✅ |
| BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation | [link] (https://arxiv.org/pdf/2201.12086) | VLM модель для генерации и понимания текста и изображений | ✅ |
| BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models | [link] (https://arxiv.org/pdf/2301.12597) | Продвинутая версия BLIP с замороженным энкодером изображений и LLM | ✅ |
| Score-CAM: Score-Weighted Visual Explanations for Convolutional Neural Networks | [link] (https://arxiv.org/pdf/1910.01279) | Визуальная карта важности для CNN, улучшение Grad-CAM | ✅ |
| Grad-CAM++: Improved Visual Explanations for Deep Convolutional Networks | [link] (https://arxiv.org/pdf/1710.11063) | Расширение Grad-CAM с более точной локализацией для CNN | ✅ |
| If CLIP Could Talk: Understanding Vision-Language Model Representations Through Their Preferred Concept Descriptions | [link] (https://aclanthology.org/2024.emnlp-main.547.pdf) | Генерация текстовых рационалов для CLIP, объяснение предсказаний | ✅ |
| Multimodal Explanations: Justifying Decisions and Pointing to the Evidence | [link] (https://arxiv.org/pdf/1802.08129) | Мультимодальные объяснения: комбинирование текста и изображений для обоснования решений модели | ✅ |
| Envisioning MedCLIP: A Deep Dive into Explainability for Medical Vision-Language Models | [link] (https://arxiv.org/pdf/2403.18996) | Объяснение VLM в медицинской области, полезно для Explainer | ✅ |
| TextCAM: Explaining Class Activation Map with Text | [link] (https://arxiv.org/pdf/2510.01004) | Комбинирование CAM и текстовых описаний для объяснения визуальных признаков | ✅ |
| Integrating Listwise Ranking into Pairwise-based Image-Text Retrieval | [link] (https://arxiv.org/pdf/2305.16566) | Ранжирование пар “изображение-текст” с улучшением через листовой подход | ✅ |
| Poly-encoders: Transformer Architectures and Pre-training Strategies for Fast and Accurate Multi-sentence Scoring | [link] (https://arxiv.org/pdf/1905.01969) | Архитектура для reranking и скоринга множественных предложений, подходит для Ranker | ✅ |
| TinyGPT-V: Efficient Multimodal Large Language Model via Small Backbones | [link] (https://arxiv.org/pdf/2312.16862) | Лёгкая мультимодальная LLM для генерации и embedding | ✅ |
| PaLI-3 Vision Language Models: Smaller, Faster, Stronger | [link] (https://arxiv.org/pdf/2310.09199) | Компактная VLM модель для retrieval и объяснений | ✅ |
| LamRA: Large Multimodal Model as Your Advanced Retrieval Assistant | [link] (https://openaccess.thecvf.com/content/CVPR2025/papers/Liu_LamRA_Large_Multimodal_Model_as_Your_Advanced_Retrieval_Assistant_CVPR_2025_paper.pdf) | Мультимодальная LLM для поиска и объяснений | ✅ |
| ECOR: Explainable CLIP for Object Recognition | [link] (https://arxiv.org/pdf/2404.12839) | Дообучение CLIP для генерации объяснений выбора объектов | ✅ |
| Explainable Saliency: Articulating Reasoning with Contextual Prioritization | [link] (https://openaccess.thecvf.com/content/CVPR2025/papers/Chen_Explainable_Saliency_Articulating_Reasoning_with_Contextual_Prioritization_CVPR_2025_paper.pdf) | Saliency + текстовые рационалы для объяснения решений | ✅ |
| Decoding the Multimodal Maze: A Systematic Review on the Adoption of Explainability in Multimodal Attention-based Models | [link] (https://arxiv.org/pdf/2508.04427) | Обзор методов XAI для мультимодальных моделей, attention-based подходы | ✅ |
| Multi-Agent Reinforcement Learning: a critical survey | [link] (https://ai.stanford.edu/~shoham/www%20papers/MALearning_ACriticalSurvey_2003_0516.pdf) | Обзор MARL, полезен для понимания архитектуры агентов | ✅ |
| ImageBind: One Embedding Space To Bind Them All | [link] (https://arxiv.org/pdf/2305.05665) | Унифицированные embeddings для нескольких модальностей, можно адаптировать для научных графиков | ✅ |
| Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks | [link] (https://arxiv.org/pdf/1908.10084) | Эффективные текстовые embeddings для similarity и Ranker | ✅ |